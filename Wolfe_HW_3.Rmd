---
title: "Module 3 Wolfe"
author: "Tyler Wolfe"
date: "9/13/2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Problem 1:

The data file jevons in the alr4 package is a study of the weight of 274 gold coins that were collected in 1868 in Manchester, England.  For each coin the weight after cleaning was recorded to the nearest 0.001 gram, and the date of the issue.  The data includes a summary of this information the variable Age, which is the age of the coin in decades, n, the number of coins with that age, Weight, the average weight of the n coins of that age, SD, the standard deviation of the n coins of that age.  The standard for new coins was 7.9876g, and legally the coins had to exceed 7.9379g. 

Use weighted least squares to analyze the data appropriately to define the relationship between weight and age.  Interpret the regression coefficients, and compare the result to using original least squares (no weights) and discuss the differences.  How do your predictions at ages 1-5 relate to the legal minimum for a coin? 

Hint:  Plot the relationship of SD to the mean to look to see if the variance is constant.  Remember the Variance of any mean is SD^2/n, which you will need to create your weight. 
----

First step is to open the data and explore the variables
```{r}
library(alr4)
help(jevons)
head(jevons)
```
We see that the head only produces 5 observations instead of the typical 6 observations. Taking a look at the dimensions, we can see the size of the data set.
```{r}
dim(jevons)
```
We see that there are only 5 observations and 6 variables. 

```{r}
plot(Weight~Age,data=jevons)
```
In the above plot, we see that there is a negative correlation characterized by the downward trend between Weight and Age, where when Age increases, Weight decreases. Understanding that the Weight variable, in relation to Age, in this dataset is an average of all of the coins of that age. For instance, the average Weight shown by Age=1 is representative of 123 observed different weighted coins that are 1 decade old. The Weight data point for Age=2 is a representatin of the average weight for 78 observed coins that are 2 decades old and etc. 

When we revisit our head from above we see that the standard deviation is also shown, as well as the minimum weight and maximum weights observed within the individual vectors, as sorted by age. For example, we can see that the minimum weight of all 123 observations in Age=1 range from 7.900 grams - 7.999 grams, but are represented in the plot above as the mean of 7.9725 iwith a Standard Deviation of .01409.

As we consider the overall implications on the data because of the inequality of the data points and the observations they represent, it may be worth looking into the variances.

```{r}
plot(SD^2~Age, data=jevons)
```
By plotting the Standard Deviation squared in comparison with Age, we see that as the age increases, so does the the variance. Because the variance is not constant (because the data points do not form a horizontal line across the plot), we will claculate the variance of each data point in Plot(Weight~Age, data=jevons). To do so we will use $SD^2/n$.

```{r}
model1<-lm(Weight~Age,weights=n/(SD^2),data=jevons)
summary(model1)
```
In the summary of our model, we see that as Age increases by one unit, we should expect weight to decrease by .02 grams. The p-value of .0001 is smaller than .05 which confirms our inference from above that Age and Weight have a relationship. Before we plot the least squares model, lets look at a simple linear regression.

```{r}
model2<-lm(Weight~Age,data=jevons)
summary(model2)
```

Comparing the two summaries, we see that there are some discrepencies. These differences in coefficients are because model1 is including a weighted least squared model accounting for non-constant variance, where model 2 is only comparing the relationship between Weights and Age assuming constant variance.

Lets plot the models to see the differences visually. 

```{r}
plot(Weight~Age,data=jevons)
abline(model1)
abline(model2,col=4)
```
As seen in the above plot, the two lines (for model1 (black) and model2 (blue)) vary only by small amounts. Because the standard deviation of the Age=1 is smaller, and slowly grows as we approach Age=5, we can be more certain that we can predict in Age=1. The variance becomes less predictable as we move to the right on the x-axis towards Age=4 and Age=5.

So lets consider when currency should be taken out of circulation to remain 95% confident that the currency still means the legal weight of 7.9379 grams. 
```{r}
predAge1=predict(model1, newdataWLS = data.frame(weight=7.9379), interval = "confidence")
predAge1
```
Concerning predictability of whether the coins maintain the legal minimum for circulation, we can be 95% confident that after one or two decades of circulation currency still meets the legal specifications. However, after two decades of circulation we see that the upper bounds of the predicted weight for Age=3, 4, and 5 illustrate that the coins are not in compliance with minimum legal specifications. By this rational, we should remove the coins after 20 years of circulation.

---------------------------------------------------------------------------

## Problem 2:  
Using the stopping data in the alr4 package, fit a model that estimates the stopping distance (Distance) given the speed is known (Speed).  Create a scatter plot, and then investigate a possible transformation that allows OLS to fit this model.  Interpret the model and present any insights you may have.
----

To begin analyzing the data, I will examine the dimensions, head and plot the data in order to understand the data, prior to determining the appropriate transformation.

```{r}
dim(stopping)
help(stopping)
```
By opening the help file associated with the data, we see that the speed is measured in mph, while distance is measured in feet.
```{r}
head(stopping)
```
```{r}
plot(stopping)
modeld<-lm(Distance~Speed, data=stopping)
abline(modeld, col=4)
```
After plotting the data, we observe that the distance interprets as higher than on order of magnitude. Under the "Log Rule," which states that if the values of a variable range by more than one order of magnitude and the variable is strictly positive, then use of a log transformation is appropriate. Since all observations of distance are positive, this applies.

```{r}
summary(modeld)
```
```{r}
modeldLog<-lm(log(Distance)~log(Speed), data=stopping)
summary(modeldLog)
```
```{r}
plot(modeldLog)
```
After transforming the model to log(Distance) as response and log(Speed) as predictor, we can see by our residuals vs. fitted plot that there is an obvious curve among our residuals. As a square root transformation is a common practice for stabilizing variance among residuals, I will attempt to execute a square root transformation model, first only transforming the response, then moving on to transform the predictor if necessary to further stabilize variance in the residuals.

```{r}
modeld3<-lm(sqrt(Distance)~Speed, data=stopping)
summary(modeld3)
```

```{r}
plot(sqrt(Distance)~Speed, data = stopping)
plot(modeld3)
```

in comparison, the new residuals vs. fitted based on the square root transformation seems to have stabilized the variance in the residuals.

We see in the summary of modeld3, that we can expect that for every one mph of speed, we can expect the necessary stopping distance to increase by .25 feet.

Unfortunately, using a square root transformation also is known to create issues for interpretations. For instance, in the above discussion concerning the relationship between speed and distance, we cannot infer that 1 mph increase will also increase stopping distance by .25ft^2 (.0625 ft) because there is no way to transform the result to an accurate number.

Hopefully soon we will learn another alternative method of stabilizing variance within residuals and conversion to linear data that does not create issues with inference. 

---------------------------------------------------------------------------

## Problem 3:  
Please look at the help file for the data in this problem.  We have been asked to analyze the MinnLand data in the alr4 package.  For now we will analyze it without using the region or financing variables included in the data (DO NOT USE THIS IN YOUR MODEL YET). Using all other variables, fit a model to explain acrePrice using all other variables.  Decide on appropriate transformations and diagnostic methods for the data. Fit your final model and provide any insights it provides on the data.  You should include any interpretation you can provide.
-----

First I will load the data and explore dimensions and identify variables in the data by examining the head.

```{r}
library(alr4)
dim(MinnLand)
```
```{r}
head(MinnLand)
help(MinnLand)
```
In the above printouts, we see that the data has 18700 observations over 9 vectors. In the head we observe that the help file was correct in saying that not all of the vectors contain exclusively values, as some counties in the state of Minnesota have not factored in productivity or had land surveyed to account for if the land is tillable by an assessor. Lets execute a take a look at the summary of the data.

```{r}
summary(MinnLand)
```

In the summary, we see that several values have a magnitude of 1 or higher so we could consider using log transformation. But first, lets execute a non-transformed linear regression.

```{r}
modela<-lm(acrePrice~improvements+year+acres+tillable+crpPct+productivity, data=MinnLand)
summary(modela)
```

After executing the model, we should look further into our residuals.

```{r}
par(mfrow=c(2,2))
plot(modela)
```
In our residuals vs. fitted plot we can see that there is a curve in our residuals, indicating that we should try to transform the data in order to execute a more accurate regression model.

So lets try a log transformation.

```{r}
modelaLog<-lm(log(acrePrice)~improvements+year+log(acres)+log(tillable)+crpPct+productivity, data=MinnLand)
summary(modelaLog)
```
By comparing the residuals of modela and modelaLog, we have successfully reduced the residuals, but lets explore a little more.

```{r}
par(mfrow=c(2,2))
plot(modelaLog)
```
Still not satisfied with the shape of the residuals vs. fitted, I will try a square root transformation.

```{r}
modelaSQRT<-lm(sqrt(acrePrice)~improvements+year+sqrt(acres)+sqrt(tillable)+crpPct+productivity, data=MinnLand)
summary(modelaSQRT)
```
```{r}
par(mfrow=c(2,2))
plot(modelaSQRT)
```
Understanding that square root transformation can create issues with interpretation, as discussed in problem 2, as well as that the shape of the line on the right side of the residuals vs. fitted plot, I have decided to interpret the data using the log model.
```{r}
summary(modelaLog)
```

given the summary of the coefficients of modelaLog, we can observe that:

A one unit increase in improvements yields a 1.4% increase in price per acre.
A one unit increase in year yields a 1.21% increase in price per acre (I feel that this may be incorrect as I am unsure how to convert year into a factor).
A one percent increase in acres yields a 1.08% decrease in price per acre.
A one percent increase in tillable land yields a 2.155% increase in price per acre.
A one unit increase in acres enrolled in the Conservation Reserve Program yields a 4.73% decrease in price per acre.
A one unit increase in average agronomic productivity yields a 1.63% increase in price per acre.

